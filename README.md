

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.



![A diagram of the process you'll follow for the project.](creating-and-optimizing-an-ml-pipeline.png)

## Summary
The data is related with direct marketing campaigns of a Portuguese banking institution made available by [UC Irvine](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing#) . The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.

The goal is to predict if a product will be subscribed by the customer contacted.

The best performing model is a *VotingEnsemble algorithm* (accuracy of 0.91769) from an AutoML.

The best result with LogisticRegression algorithm trained through Azure HyperDrive is a model with accuracy score of 0.9098 (with C= 8.341, max_iter=50)

## Scikit-learn Pipeline
The first model is a SciKitLearn LogRegression model trained using an *HyperDrive* in a Azure ML pipeline. The model is defined  the *train.py*.

1. A *TabularDataset* is created using TabularDatasetFactory class from the dataset CSV file.
2.  Daset cleaning:
   1. Drop each row containing missing values
   2. Convert categorical variable to into one hot encoding dummy indicator
      - Convert the *job*, *contact* and *education* variables into dummy/indicator variables, removing then the original ones
      - Convert the categorical variables *marital*, *default*, *housing*, *loan* and *poutcome* in a numeric (dummy) one (1 and 0)
   3. Encoding the *month* and *day_of_week* variables into integers  using the map function (ordinal encoding)
3. A  dataframe containing  containing just the target variable (y) is extracted from the dataframe containing all the features (x)
4. The dataset is split in *train* and *test* data (75%-25%) using a random sampling of the rows.
5. A Logistic Regression model is fitted using the values of the parameters *C* and *max_iter* generated by HyperDrive
6. Parameters value and Accuracy score are logged into the context run
7. The fitted model is saved in a file using  into a file using job lib

### HyperDrive Parameter Sampler

Azure Machine Learning supports the following parameter sampling methods:

- Random sampling
- Grid sampling
- Bayesian sampling

*Random Sampling* is used is proven to be more efficient with little performance loss respect to *grid sampling*.  

### Early Stop Policy

The early stop policy terminate poorly performing runs in order to improves computational efficiency. 

The chosen early stopping policy is the *BanditPolicy*. Bandit terminates runs where the primary metric is not within the specified slack factor/slack amount compared to the best performing run.

The policy is configured with the  with the following settings:

- check interval: 1 iteration 
- slack factor: 0.2
- delay evaluation: 5

If the accuracy falls outside of the top 20% range, Azure ML terminate the job. This avoids waste of computational resorces with poorly performing runs.

## AutoML
The final model generated by AutoML is a *voting ensemble* made by different models:

```
'0': Pipeline(memory=None,
          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                 ('lightgbmclassifier',
                  LightGBMClassifier(boosting_type='gbdt', class_weight=None,
                                     colsample_bytree=1.0,
                                     importance_type='split', learning_rate=0.1,
                                     max_depth=-1, min_child_samples=20,
                                     min_child_weight=0.001, min_split_gain=0.0,
                                     n_estimators=100, n_jobs=1, num_leaves=31,
                                     objective=None, random_state=None,
                                     reg_alpha=0.0, reg_lambda=0.0, silent=True,
                                     subsample=1.0, subsample_for_bin=200000,
                                     subsample_freq=0, verbose=-10))],
          verbose=False),
 '1': Pipeline(memory=None,
          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                 ('xgboostclassifier',
                  XGBoostClassifier(base_score=0.5, booster='gbtree',
                                    colsample_bylevel=1, colsample_bynode=1,
                                    colsample_bytree=1, gamma=0,
                                    learning_rate=0.1, max_delta_step=0,
                                    max_depth=3, min_child_weight=1, missing=nan,
                                    n_estimators=100, n_jobs=1, nthread=None,
                                    objective='binary:logistic', random_state=0,
                                    reg_alpha=0, reg_lambda=1,
                                    scale_pos_weight=1, seed=None, silent=None,
                                    subsample=1, tree_method='auto', verbose=-10,
                                    verbosity=0))],
          verbose=False),
 '24': Pipeline(memory=None,
          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                 ('lightgbmclassifier',
                  LightGBMClassifier(boosting_type='goss', class_weight=None,
                                     colsample_bytree=0.5944444444444444,
                                     importance_type='split',
                                     learning_rate=0.04211105263157895,
                                     max_bin=330, max_depth=4,
                                     min_child_samples=1251, min_child_weight=6,
                                     min_split_gain=0.9473684210526315,
                                     n_estimators=200, n_jobs=1, num_leaves=230,
                                     objective=None, random_state=None,
                                     reg_alpha=0.894736842105263,
                                     reg_lambda=0.3157894736842105, silent=True,
                                     subsample=1, subsample_for_bin=200000,
                                     subsample_freq=0, verbose=-10))],
          verbose=False),
 '22': Pipeline(memory=None,
          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                 ('lightgbmclassifier',
                  LightGBMClassifier(boosting_type='gbdt', class_weight=None,
                                     colsample_bytree=0.8911111111111111,
                                     importance_type='split',
                                     learning_rate=0.09473736842105263,
                                     max_bin=70, max_depth=6,
                                     min_child_samples=2614, min_child_weight=1,
                                     min_split_gain=0.7368421052631579,
                                     n_estimators=50, n_jobs=1, num_leaves=170,
                                     objective=None, random_state=None,
                                     reg_alpha=0.5789473684210527,
                                     reg_lambda=0.6842105263157894, silent=True,
                                     subsample=0.5942105263157895,
                                     subsample_for_bin=200000, subsample_freq=0,
                                     verbose=-10))],
          verbose=False),
 '19': Pipeline(memory=None,
          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                 ('sgdclassifierwrapper',
                  SGDClassifierWrapper(alpha=0.8164183673469387,
                                       class_weight=None, eta0=0.01,
                                       fit_intercept=False,
                                       l1_ratio=0.14285714285714285,
                                       learning_rate='invscaling',
                                       loss='modified_huber', max_iter=1000,
                                       n_jobs=1, penalty='none',
                                       power_t=0.1111111111111111,
                                       random_state=None, tol=0.001))],
          verbose=False),
 '4': Pipeline(memory=None,
          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                 ('sgdclassifierwrapper',
                  SGDClassifierWrapper(alpha=9.59184081632653,
                                       class_weight='balanced', eta0=0.01,
                                       fit_intercept=True,
                                       l1_ratio=0.3877551020408163,
                                       learning_rate='invscaling', loss='log',
                                       max_iter=1000, n_jobs=1, penalty='none',
                                       power_t=0, random_state=None,
                                       tol=0.01))],
          verbose=False),
 '21': Pipeline(memory=None,
          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                 ('randomforestclassifier',
                  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                         class_weight='balanced',
                                         criterion='entropy', max_depth=None,
                                         max_features='sqrt',
                                         max_leaf_nodes=None, max_samples=None,
                                         min_impurity_decrease=0.0,
                                         min_impurity_split=None,
                                         min_samples_leaf=0.035789473684210524,
                                         min_samples_split=0.01,
                                         min_weight_fraction_leaf=0.0,
                                         n_estimators=10, n_jobs=1,
                                         oob_score=True, random_state=None,
                                         verbose=0, warm_start=False))],
          verbose=False),
 '20': Pipeline(memory=None,
          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),
                 ('extratreesclassifier',
                  ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0,
                                       class_weight='balanced', criterion='gini',
                                       max_depth=None, max_features='log2',
                                       max_leaf_nodes=None, max_samples=None,
                                       min_impurity_decrease=0.0,
                                       min_impurity_split=None,
                                       min_samples_leaf=0.01,
                                       min_samples_split=0.01,
                                       min_weight_fraction_leaf=0.0,
                                       n_estimators=50, n_jobs=1, oob_score=True,
                                       random_state=None, verbose=0,
                                       warm_start=False))],
          verbose=False)}
          
          Weights:   
[0.4666666666666667,
 0.06666666666666667,
 0.06666666666666667,
 0.06666666666666667,
 0.06666666666666667,
 0.06666666666666667,
 0.06666666666666667,
 0.13333333333333333]
```

![image-20201128015333647](auto_ml_model_explain.png)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The two model are the result of two different process:

- **HyperDrive** on LogisticRegression:  trains the LogisticRegression model using different tuples of hyper-parameters. 

  ![sk learn experiment](sklearn_experiment.png)

- **AutoML** runs executes complex pipelines with  different combination of hyper-parameters, models and configuration details. 

  ![image-20201127161219698](auto_ml_experiment_completed.png)

AutoML can also use both liner and non linear ML model.

## Future work

### Improve HyperDrive settings

Refine hyperparameter space definition starting with *random sampling* and the switch to *grid sampling* in a narrow search space.

Try to use bayesian sampling 

### Deal with unbalanced training data

A different *primary metric* shoud be used to take in account the fact that the daset is heavely unbalenced.  AUC_weighted may be abetter choice.

![image-20201128010145588](unbalanced_dataset_y.png)



### Feature Engeneering

There are some numerical columns in the dataset with a elevated number o distinc values. The values shoud be grouped in bins and colums should be converted to categorical

Less relevant features shoud be removed to simplify the model

The most relevant feature for the prediction *duration* . This information is available when the last call to the prospect customer is concluded. A different model for early prediction shoud be developed.

### Improve Auto ML Configuration

Increse "experiment timeout" in AutoML config

Enable deep learning in classification task